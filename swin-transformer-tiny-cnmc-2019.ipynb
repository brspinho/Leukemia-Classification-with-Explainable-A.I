{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9384155,"sourceType":"datasetVersion","datasetId":5693373}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import gdown\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport shutil\nimport os\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport timm\nimport torch\nimport torch.nn.functional as F\n\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, ConcatDataset, random_split\nfrom torchvision import models, transforms\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:11:35.161885Z","iopub.execute_input":"2025-11-21T15:11:35.162141Z","iopub.status.idle":"2025-11-21T15:11:45.693735Z","shell.execute_reply.started":"2025-11-21T15:11:35.162120Z","shell.execute_reply":"2025-11-21T15:11:45.692989Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"# Plots 9 random images from the dataset\n\ndef plot_random_images(dataset, gray=False):\n    random_idx = np.random.randint(0, len(dataset), 9)\n\n    plt.figure(figsize=(10, 10))\n    for i, img_index in enumerate(random_idx):\n        plt.subplot(3,3,i+1)\n        plt.grid(False)\n        image, label = dataset[img_index]\n        plt.title(label)\n        if gray:\n          plt.imshow(image.permute(1,2,0), cmap='gray')\n        else:\n          plt.imshow(image.permute(1,2,0))\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:11:52.896079Z","iopub.execute_input":"2025-11-21T15:11:52.896457Z","iopub.status.idle":"2025-11-21T15:11:52.901422Z","shell.execute_reply.started":"2025-11-21T15:11:52.896431Z","shell.execute_reply":"2025-11-21T15:11:52.900675Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Plot confusion matrix\n\ndef plot_confusion_matrix(cm, class_names=[0, 1]):\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:12:23.646349Z","iopub.execute_input":"2025-11-21T15:12:23.647054Z","iopub.status.idle":"2025-11-21T15:12:23.650746Z","shell.execute_reply.started":"2025-11-21T15:12:23.647031Z","shell.execute_reply":"2025-11-21T15:12:23.650024Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Auxiliary function for train_classification()\n\ndef _train_step_classification(classifier_model, dataloader, criterion, optimizer, scheduler, device):\n    classifier_model.train()\n\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    for X, y in dataloader:\n        X, y = X.to(device), y.to(device)\n        y_pred = classifier_model(X)\n        loss = criterion(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * X.size(0)\n\n        y_pred_labels = torch.argmax(y_pred, axis=1)\n        all_preds.extend(y_pred_labels.detach().cpu().numpy())\n        all_targets.extend(y.detach().cpu().numpy())\n\n    if scheduler:\n        scheduler.step()\n\n    mean_loss = total_loss / len(dataloader.dataset)\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, average='binary', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='binary', zero_division=0)\n    f1 = f1_score(all_targets, all_preds, average='binary', zero_division=0)\n\n    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n\n    return mean_loss, accuracy, precision, recall, f1, sensitivity, specificity\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Auxiliary function for train_classification()\n\ndef _test_step_classification(classifier_model, dataloader, criterion, device):\n    classifier_model.eval()\n\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            y_pred = classifier_model(X)\n            loss = criterion(y_pred, y)\n\n            total_loss += loss.item() * X.size(0)\n\n            y_pred_labels = torch.argmax(y_pred, axis=1)\n            all_preds.extend(y_pred_labels.detach().cpu().numpy())\n            all_targets.extend(y.detach().cpu().numpy())\n\n    mean_loss = total_loss / len(dataloader.dataset)\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, average='binary', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='binary', zero_division=0)\n    f1 = f1_score(all_targets, all_preds, average='binary', zero_division=0)\n    cm = confusion_matrix(all_targets, all_preds)\n\n    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n\n    return mean_loss, accuracy, precision, recall, f1, sensitivity, specificity, cm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trains classification model\n\ndef train_classification(classifier_model, train_dataloader, test_dataloader, criterion, epochs, optimizers, schedulers, device='cpu', verbose=True):\n    metrics = {'Train loss': [], 'Train accuracy': [], 'Train sensitivity': [], 'Train specificity': [], 'Train precision': [], 'Train recall': [], 'Train F1': [],\n               'Test loss': [], 'Test accuracy': [], 'Test sensitivity': [], 'Test specificity': [], 'Test precision': [], 'Test recall': [], 'Test F1': []}\n\n    current_optimizer_number = 0\n    for optimizer, scheduler, num_epochs in zip(optimizers, schedulers, epochs):\n        current_optimizer_number += 1\n        for epoch in tqdm(range(1, num_epochs+1), desc=f'Training with optimizer {current_optimizer_number}'):\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            train_loss, train_accuracy, train_precision, train_recall, train_f1, train_sensitivity, train_specificity = _train_step_classification(classifier_model, train_dataloader, criterion, optimizer, scheduler, device)\n            test_loss, test_accuracy, test_precision, test_recall, test_f1, test_sensitivity, test_specificity, cm = _test_step_classification(classifier_model, test_dataloader, criterion, device)\n\n            metrics['Train loss'].append(train_loss)\n            metrics['Train accuracy'].append(train_accuracy)\n            metrics['Train sensitivity'].append(train_sensitivity)\n            metrics['Train specificity'].append(train_specificity)\n            metrics['Train precision'].append(train_precision)\n            metrics['Train recall'].append(train_recall)\n            metrics['Train F1'].append(train_f1)\n            metrics['Test loss'].append(test_loss)\n            metrics['Test accuracy'].append(test_accuracy)\n            metrics['Test sensitivity'].append(test_sensitivity)\n            metrics['Test specificity'].append(test_specificity)\n            metrics['Test precision'].append(test_precision)\n            metrics['Test recall'].append(test_recall)\n            metrics['Test F1'].append(test_f1)\n\n            if verbose:\n                print(f'\\nEPOCH {epoch} | Current learning rate: {current_lr:.8f}\\n'\n                      f'Train loss: {train_loss:.4f} | Train accuracy: {train_accuracy:.4f} | Train sensitivity: {train_sensitivity:.4f} | Train specificity: {train_specificity:.4f} | Train precision: {train_precision:.4f} | Train recall: {train_recall:.4f} | Train F1: {train_f1:.4f}\\n'\n                      f'Test loss: {test_loss:.4f} | Test accuracy: {test_accuracy:.4f} | Test sensitivity: {test_sensitivity:.4f} | Test specificity: {test_specificity:.4f} | Test precision: {test_precision:.4f} | Test recall: {test_recall:.4f} | Test F1: {test_f1:.4f}\\n')\n\n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_classification(classifier_model, test_dataloader, criterion, device='cpu', verbose=True):\n    metrics = {'Test loss': [], 'Test accuracy': [], 'Test sensitivity': [], 'Test specificity': [], 'Test precision': [], 'Test recall': [], 'Test F1': [], 'Confusion matrix': None}\n\n    test_loss, test_accuracy, test_precision, test_recall, test_f1, test_sensitivity, test_specificity, cm = _test_step_classification(\n        classifier_model, test_dataloader, criterion, device\n    )\n\n    metrics['Test loss'].append(test_loss)\n    metrics['Test accuracy'].append(test_accuracy)\n    metrics['Test sensitivity'].append(test_sensitivity)\n    metrics['Test specificity'].append(test_specificity)\n    metrics['Test precision'].append(test_precision)\n    metrics['Test recall'].append(test_recall)\n    metrics['Test F1'].append(test_f1)\n    metrics['Confusion matrix'] = cm\n\n    if verbose:\n        print(f'Test loss: {test_loss:.4f} | Test accuracy: {test_accuracy:.4f} | '\n              f'Test sensitivity: {test_sensitivity:.4f} | Test specificity: {test_specificity:.4f} |'\n              f'Test precision: {test_precision:.4f} | Test recall: {test_recall:.4f} | '\n              f'Test F1: {test_f1:.4f}\\n')\n\n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Returns confusion matrix\n\ndef get_conf_matrix(classifier_model, dataloader, device):\n    classifier_model.eval()\n\n    all_preds = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            y_pred = classifier_model(X).argmax(dim=1).cpu()\n\n            all_preds.extend(y_pred.cpu())\n            all_targets.extend(y.cpu())\n\n    conf_matrix = confusion_matrix(all_targets, all_preds)\n\n    return conf_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plots metrics as graphs\ndef plot_metrics(metrics):\n    epochs = range(1, len(metrics['train_loss']) + 1)\n\n    plt.figure(figsize=(15, 5))\n\n    # --- Loss ---\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, metrics['train_loss'], label='Training')\n    plt.plot(epochs, metrics['val_loss'], label='Validation')\n    plt.title(\"Loss\")\n    plt.xlabel(\"Ephocs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    # --- Accuracy ---\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, metrics['train_accuracy'], label='Training')\n    plt.plot(epochs, metrics['val_accuracy'], label='Validation')\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Ephocs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n\n    # --- F1 ---\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, metrics['train_f1'], label='Training')\n    plt.plot(epochs, metrics['val_f1'], label='Validation')\n    plt.title(\"F1\")\n    plt.xlabel(\"Ephocs\")\n    plt.ylabel(\"F1\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plots metrics as indivual graphs in .svg\nimport matplotlib.pyplot as plt\n\ndef save_separate_metrics_plots_svg(metrics):\n    epochs = range(1, len(metrics['train_loss']) + 1)\n    \n    FIGSIZE = (5, 5) \n    \n    # --- Loss ---\n    plt.figure(figsize=FIGSIZE)\n    plt.plot(epochs, metrics['train_loss'], label='Training')\n    plt.plot(epochs, metrics['val_loss'], label='Validation')\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig('metrics_loss_plot_proportional.svg', format='svg') \n    plt.close()\n    \n    # --- Accuracy ---\n    plt.figure(figsize=FIGSIZE)\n    plt.plot(epochs, metrics['train_accuracy'], label='Training')\n    plt.plot(epochs, metrics['val_accuracy'], label='Validation')\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig('metrics_accuracy_plot_proportional.svg', format='svg')\n    plt.close()\n\n    # --- F1 ---\n    plt.figure(figsize=FIGSIZE)\n    plt.plot(epochs, metrics['train_f1'], label='Training')\n    plt.plot(epochs, metrics['val_f1'], label='Validation')\n    plt.title(\"F1 Score\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"F1 Score\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig('metrics_f1_plot_proportional.svg', format='svg')\n    plt.close()\n    \n    return ['metrics_loss_plot_proportional.svg', 'metrics_accuracy_plot_proportional.svg', 'metrics_f1_plot_proportional.svg']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wrapper to apply transform on subset\nclass TransformedDataset(Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset   \n        self.transform = transform\n\n    def __getitem__(self, idx):\n        x, y = self.subset[idx] \n        if self.transform:\n            x = self.transform(x)\n        return x, y\n\n    def __len__(self):\n        return len(self.subset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Managing the Dataset","metadata":{}},{"cell_type":"markdown","source":" **Adjusting labels, applying augmentations, preprocessing, splitting into the correct sizes for the respective sets**","metadata":{}},{"cell_type":"code","source":"# Setting path and renaming labels\n\nimport shutil, os\n\n# Source path\nsrc = \"/kaggle/input/c-nmc-2019-dataset/C-NMC 2019 (PKG)/C-NMC_training_data\"\n\n# Destiny path\ndst = \"/kaggle/working/C-NMC_training_data\"\n\n# Copies the entire folder for working\nif not os.path.exists(dst):\n    shutil.copytree(src, dst)\n\n# We can now rename the folders inside each fold\nfor fold in [\"fold_0\", \"fold_1\", \"fold_2\"]:\n    fold_path = os.path.join(dst, fold)\n\n    # Rename ALL → 1\n    all_path = os.path.join(fold_path, \"all\")\n    if os.path.exists(all_path):\n        os.rename(all_path, os.path.join(fold_path, \"1\"))\n\n    # Rename HEM → 0\n    hem_path = os.path.join(fold_path, \"hem\")\n    if os.path.exists(hem_path):\n        os.rename(hem_path, os.path.join(fold_path, \"0\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining Augmentations\n\ntrain_transform = transforms.Compose([\n    transforms.CenterCrop(224),               # necessary for the swin input\n    transforms.RandomHorizontalFlip(p=0.5),      # only training\n    transforms.RandomVerticalFlip(p=0.5),        # only training\n    transforms.RandomAffine(degrees=30, translate=(0.00, 0.00), scale=(1.00, 1.00), shear=5), # only training\n    transforms.ToTensor(),\n])\n\nval_transform = transforms.Compose([\n    transforms.CenterCrop(224),  # necessary for the swin input\n    transforms.ToTensor(),\n])\n\ntest_transform = transforms.Compose([\n    transforms.CenterCrop(224),  # necessary for the swin input\n    transforms.ToTensor(),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatanating folders\n\nfold_0 = ImageFolder(os.path.join(dst, \"fold_0\"))\nfold_1 = ImageFolder(os.path.join(dst, \"fold_1\"))\nfold_2 = ImageFolder(os.path.join(dst, \"fold_2\"))\n\ndataset = ConcatDataset([fold_0, fold_1, fold_2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting sets for train, validation and test (70%, 20%, 10%)\n\nseed = 42\ng = torch.Generator().manual_seed(seed)\n\n# Sizes\nn_total = len(dataset)\nn_test = int(0.10 * n_total)\nn_val  = int(0.20 * n_total)\nn_train = n_total - n_val - n_test\n\n# Split\ntrain_dataset, val_dataset, test_dataset = random_split(\n    dataset, [n_train, n_val, n_test], generator=g\n)\n\nprint(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the respective transformations\n\ntrain_dataset = TransformedDataset(train_dataset, transform=train_transform)\nval_dataset   = TransformedDataset(val_dataset,   transform=val_transform)\ntest_dataset  = TransformedDataset(test_dataset,  transform=test_transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WeightedRandomSampler","metadata":{}},{"cell_type":"code","source":"# Recalculate weights for the WeightedRandomSampler with the new training set\n\nprint(\"\\nRecalculating weights for the sampler in the new train set...\")\ntargets = [y for _, y in train_dataset]\n\nclass_sample_counts = np.bincount(targets)\nweights = 1. / class_sample_counts\nsample_weights = [weights[t] for t in targets]\n\n# Balanced sampler\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=30000, # Samples the same number of images as the train dataset\n    replacement=True\n)\nprint(\"Weight successfully recalculated.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Let's see some of the dataset images","metadata":{}},{"cell_type":"code","source":"plot_random_images(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the Swin Transformer","metadata":{}},{"cell_type":"code","source":"class SwinTransformerTiny(nn.Module):\n    def __init__(self, drop_path_rate=0, drop_rate=0, attn_drop_rate=0):\n        super().__init__()\n        self.model = timm.create_model(\n            'swin_tiny_patch4_window7_224',\n            pretrained=True,\n            num_classes=2,\n            drop_path_rate=drop_path_rate,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate\n        )\n\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Removing the attention mechanism (in case of an Ablation Study)","metadata":{}},{"cell_type":"code","source":"# If you wish to remove the attention mechanism for the ablation study\nimport torch.nn as nn\n\n'''\ndef remove_attention(model):\n\n    for name, module in model.named_children():\n\n        # If the module is an attention layer\n        if \"attn\" in name.lower():\n            setattr(model, name, nn.Identity())\n        \n        # If its a SwinTransformerBlock (with function _attn)\n        elif hasattr(module, \"_attn\"):\n            def no_attn(x, mask=None):\n                return x\n            module._attn = no_attn\n\n        # Recursively replaces inside the submodules\n        else:\n            remove_attention(module)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the ResNet18 (experimental purposes, only)","metadata":{}},{"cell_type":"code","source":"# If you wish to use Resnet18 for testing or etc\n\n\"\"\"class ResNet18(nn.Module):\n    def __init__(self, drop_path_rate=0, drop_rate=0):\n        super().__init__()\n        self.model = timm.create_model(\n            'resnet18',\n            pretrained=True,\n            num_classes=2,\n            drop_path_rate=drop_path_rate,\n            drop_rate=drop_rate\n        )\n\n    def forward(self, x):\n        return self.model(x)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Forcing to use dedicated GPU","metadata":{}},{"cell_type":"code","source":"# CPU or GPU device\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloaders for train, validation and test","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=128, sampler=sampler, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)\nval_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Instantiating the model","metadata":{}},{"cell_type":"code","source":"\n\nclassifier_model = SwinTransformerTiny(drop_path_rate=0.15, drop_rate=0.1, attn_drop_rate=0.0).to(device)\n\n# Removes attention\n#remove_attention(classifier_model)\n\n#Resnet18 model\n#classifier_model = ResNet18(drop_path_rate=0.15, drop_rate=0.1).to(device)\n\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\noptimizer = optim.AdamW(classifier_model.parameters(), lr=1e-4, weight_decay=5e-2)\nscheduler = optim.lr_scheduler.StepLR(optimizer, 1, 0.9)\n\noptimizers = [optimizer]\nschedulers = [scheduler]\nepochs = [50]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downloading and loading parameters (if necessary)","metadata":{}},{"cell_type":"code","source":"# Download parameters from google drive (for retraining)\n\ngdown.download('https://drive.google.com/file/d/1b4C0jr9QEnslJketPI9IkWPZVXivjLqr/view?usp=sharing', 'parameters.pth', fuzzy=True, quiet=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load parameters\n\nclassifier_model.load_state_dict(torch.load('/kaggle/working/parameters.pth'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running the model","metadata":{}},{"cell_type":"code","source":"# Training\nmetrics = train_classification(classifier_model, train_dataloader, val_dataloader, criterion, epochs, optimizers, schedulers, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Validation\nval_metrics = test_classification(classifier_model, val_dataloader, criterion, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test\ntest_metrics = test_classification(classifier_model, test_dataloader, criterion, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Let's plot some results","metadata":{}},{"cell_type":"code","source":"#Plotting metrics\n\nplot_metrics(metrics)\n# or\n#save_separate_metrics_plots_svg(metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting confusion matrix\nplot_confusion_matrix(test_metrics['Confusion matrix'], class_names=[0, 1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Last but not least, save parameters","metadata":{}},{"cell_type":"code","source":"#Save parameters\ntorch.save(classifier_model.state_dict(), 'parameters.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}